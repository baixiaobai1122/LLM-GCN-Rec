"""
ä¸º amazon-book-2023 æ•°æ®é›†è·å–ä¹¦ç±å…ƒæ•°æ®
ä» amazon_item.json è¯»å– ASINï¼Œä½¿ç”¨ Open Library API è·å–ä¹¦ç±ä¿¡æ¯
è¾“å‡º: book_metadata.json
"""
import requests
import json
import time
from tqdm import tqdm
import os
from pathlib import Path

class BookMetadataFetcher2023:
    def __init__(self, data_dir='../../datasets/amazon-book-2023'):
        self.data_dir = Path(data_dir)
        self.cache_file = self.data_dir / 'book_metadata.json'
        os.makedirs(self.data_dir, exist_ok=True)
        self.metadata = self.load_cache()

    def load_cache(self):
        """åŠ è½½å·²ç¼“å­˜çš„å…ƒæ•°æ®"""
        if self.cache_file.exists():
            print(f"ğŸ“‚ Loading cached metadata from {self.cache_file}")
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_cache(self):
        """ä¿å­˜å…ƒæ•°æ®åˆ°ç¼“å­˜"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Saved metadata to {self.cache_file}")

    def fetch_from_openlibrary_by_isbn(self, isbn_or_asin):
        """
        ä» Open Library API è·å–ä¹¦ç±ä¿¡æ¯ (ä½¿ç”¨ ISBN)
        API: https://openlibrary.org/api/books
        """
        try:
            # å°è¯• ISBN æŸ¥è¯¢
            url = f"https://openlibrary.org/api/books?bibkeys=ISBN:{isbn_or_asin}&format=json&jscmd=data"
            response = requests.get(url, timeout=5)

            if response.status_code == 200:
                data = response.json()
                key = f"ISBN:{isbn_or_asin}"

                if key in data and data[key]:
                    return self._extract_metadata(data[key], isbn_or_asin)

            return None

        except Exception as e:
            # print(f"âŒ Error fetching {isbn_or_asin}: {e}")
            return None

    def fetch_from_openlibrary_search(self, asin):
        """
        ä½¿ç”¨ Open Library Search API æŸ¥è¯¢
        å½“ ISBN lookup å¤±è´¥æ—¶ä½¿ç”¨æ­¤æ–¹æ³•
        """
        try:
            # ä½¿ç”¨ search API
            url = f"https://openlibrary.org/search.json?q={asin}&limit=1"
            response = requests.get(url, timeout=5)

            if response.status_code == 200:
                data = response.json()

                if data.get('numFound', 0) > 0 and len(data.get('docs', [])) > 0:
                    book = data['docs'][0]

                    # æå–ä¿¡æ¯
                    metadata = {
                        'isbn': asin,
                        'title': book.get('title', 'Unknown'),
                        'authors': book.get('author_name', ['Unknown']),
                        'publish_date': str(book.get('first_publish_year', '')),
                        'publishers': book.get('publisher', [])[:3],  # å–å‰3ä¸ª
                        'subjects': [{'name': s, 'url': ''} for s in book.get('subject', [])[:10]],
                        'cover_url': f"https://covers.openlibrary.org/b/id/{book.get('cover_i', '')}-M.jpg" if book.get('cover_i') else None,
                        'description': self._format_subjects_as_description(book.get('subject', [])[:5]),
                    }

                    return metadata

            return None

        except Exception as e:
            # print(f"âŒ Search error for {asin}: {e}")
            return None

    def _extract_metadata(self, book_data, isbn_or_asin):
        """ä» Open Library æ•°æ®æå–å…ƒæ•°æ®"""
        metadata = {
            'isbn': isbn_or_asin,
            'title': book_data.get('title', 'Unknown'),
            'authors': [author.get('name', '') for author in book_data.get('authors', [])],
            'publish_date': book_data.get('publish_date', ''),
            'publishers': [p.get('name', '') for p in book_data.get('publishers', [])],
            'subjects': book_data.get('subjects', [])[:10],  # åªå–å‰10ä¸ªä¸»é¢˜
            'cover_url': book_data.get('cover', {}).get('medium', None),
            'description': self._extract_description(book_data),
        }
        return metadata

    def _extract_description(self, book_data):
        """æå–ä¹¦ç±æè¿°"""
        # Open Library æœ‰å¤šç§æè¿°æ ¼å¼
        if 'description' in book_data:
            desc = book_data['description']
            if isinstance(desc, dict) and 'value' in desc:
                return desc['value']
            elif isinstance(desc, str):
                return desc

        # å¦‚æœæ²¡æœ‰æè¿°ï¼Œä½¿ç”¨ä¸»é¢˜ä½œä¸ºæ›¿ä»£
        subjects = book_data.get('subjects', [])[:5]
        return self._format_subjects_as_description(subjects)

    def _format_subjects_as_description(self, subjects):
        """å°†ä¸»é¢˜æ ¼å¼åŒ–ä¸ºæè¿°"""
        if subjects:
            subject_names = [s.get('name', s) if isinstance(s, dict) else s for s in subjects]
            return f"Topics: {', '.join(subject_names)}"
        return "No description available"

    def fetch_all_books(self, asin_list, batch_size=100, delay=0.5):
        """
        æ‰¹é‡è·å–ä¹¦ç±å…ƒæ•°æ®

        Args:
            asin_list: [(iid, asin), ...] åˆ—è¡¨
            batch_size: æ¯ä¸ªbatchä¿å­˜ä¸€æ¬¡
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…è¢«é™æµ
        """
        total = len(asin_list)
        fetched = 0
        failed = []

        # åˆ›å»º ITEM_X åˆ° metadata çš„æ˜ å°„
        result_metadata = {}

        print(f"ğŸ“š Fetching metadata for {total} books...")
        print(f"   Already cached: {len(self.metadata)}")

        for i, (iid, asin) in enumerate(tqdm(asin_list, desc="Fetching books")):
            item_key = f"ITEM_{iid}"

            # è·³è¿‡å·²ç¼“å­˜çš„
            if item_key in self.metadata:
                result_metadata[item_key] = self.metadata[item_key]
                continue

            # å…ˆå°è¯• ISBN lookup
            metadata = self.fetch_from_openlibrary_by_isbn(asin)

            # å¦‚æœå¤±è´¥ï¼Œå°è¯• search API
            if not metadata:
                time.sleep(0.2)  # çŸ­æš‚å»¶è¿Ÿ
                metadata = self.fetch_from_openlibrary_search(asin)

            if metadata:
                result_metadata[item_key] = metadata
                self.metadata[item_key] = metadata
                fetched += 1
            else:
                # åˆ›å»ºæœ€å°å…ƒæ•°æ®ä½œä¸º fallback
                result_metadata[item_key] = {
                    'isbn': asin,
                    'title': f"Book {iid}",
                    'authors': ['Unknown'],
                    'publish_date': '',
                    'publishers': [],
                    'subjects': [],
                    'cover_url': None,
                    'description': f"Amazon ASIN: {asin}"
                }
                self.metadata[item_key] = result_metadata[item_key]
                failed.append((iid, asin))

            # å®šæœŸä¿å­˜
            if (i + 1) % batch_size == 0:
                self.save_cache()
                print(f"\n   Progress: {i+1}/{total}, Fetched: {fetched}, Failed: {len(failed)}")

            # å»¶è¿Ÿï¼Œé¿å…é™æµ
            time.sleep(delay)

        # æœ€ç»ˆä¿å­˜
        self.save_cache()

        print(f"\nâœ… Fetching complete!")
        print(f"   Total fetched: {fetched}")
        print(f"   Total in cache: {len(self.metadata)}")
        print(f"   Failed: {len(failed)}")

        if failed:
            print(f"\nâŒ Failed ASINs (first 10): {failed[:10]}")
            failed_file = self.data_dir / 'failed_asins.txt'
            with open(failed_file, 'w') as f:
                for iid, asin in failed:
                    f.write(f"{iid}\t{asin}\n")
            print(f"   Failed ASINs saved to: {failed_file}")

        return self.metadata


def load_asin_list(data_dir='../../datasets/amazon-book-2023'):
    """ä» amazon_item.json åŠ è½½ ASIN åˆ—è¡¨"""
    data_dir = Path(data_dir)
    item_file = data_dir / 'amazon_item.json'

    asin_list = []
    with open(item_file, 'r') as f:
        for line in f:
            item = json.loads(line.strip())
            asin_list.append((item['iid'], item['asin']))

    print(f"ğŸ“– Loaded {len(asin_list)} ASINs from {item_file}")
    return asin_list


def main():
    """ä¸»æµç¨‹"""
    import argparse

    parser = argparse.ArgumentParser(description="Fetch book metadata for amazon-book-2023")
    parser.add_argument('--data_dir', type=str,
                       default='../../datasets/amazon-book-2023',
                       help='Data directory path')
    parser.add_argument('--batch_size', type=int, default=100,
                       help='Save cache every N books')
    parser.add_argument('--delay', type=float, default=0.5,
                       help='Delay between requests (seconds)')
    parser.add_argument('--limit', type=int, default=None,
                       help='Limit number of books to fetch (for testing)')

    args = parser.parse_args()

    print("="*60)
    print("Amazon Book 2023 Metadata Fetcher")
    print("="*60)

    # åŠ è½½ ASIN åˆ—è¡¨
    asin_list = load_asin_list(args.data_dir)

    if args.limit:
        print(f"âš ï¸  Limiting to first {args.limit} books for testing")
        asin_list = asin_list[:args.limit]

    # åˆ›å»º fetcher å¹¶è·å–å…ƒæ•°æ®
    fetcher = BookMetadataFetcher2023(args.data_dir)
    metadata = fetcher.fetch_all_books(
        asin_list,
        batch_size=args.batch_size,
        delay=args.delay
    )

    print("\n" + "="*60)
    print("âœ“ Metadata fetching completed!")
    print(f"  Output: {fetcher.cache_file}")
    print(f"  Total items: {len(metadata)}")
    print("="*60)


if __name__ == '__main__':
    main()